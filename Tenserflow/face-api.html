<!DOCTYPE html>
<html>

<head>
  <title>Custom Face Recognition Demo</title>
  <script src="https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/dist/face-api.min.js"></script>

  <style>
    #container {
      position: relative;
      width: 640px;
      margin: auto;
    }

    video,
    canvas {
      width: 100%;
      height: 480px;
      border-radius: 8px;
      display: block;
    }

    canvas {
      position: absolute;
      left: 0;
      top: 0;
      pointer-events: none;
    }

    #nameInput,
    #addFaceBtn,
    #switchCamBtn {
      margin: 10px 5px;
      font-size: 16px;
    }
  </style>
</head>

<body>
  <h2>Custom Face Recognition</h2>
  <div id="container">
    <video id="video" autoplay muted playsinline></video>
    <canvas id="overlay"></canvas>
  </div>

  <input type="text" id="nameInput" placeholder="Enter friend's name" />
  <button id="addFaceBtn">Add Face</button>
  <button id="switchCamBtn">Switch Camera</button>
  <p id="status">Loading models...</p>

  <script>
    const video = document.getElementById('video');
    const overlay = document.getElementById('overlay');
    const ctx = overlay.getContext('2d');
    const nameInput = document.getElementById('nameInput');
    const addFaceBtn = document.getElementById('addFaceBtn');
    const switchCamBtn = document.getElementById('switchCamBtn');
    const status = document.getElementById('status');

    let labeledDescriptors = [];
    let faceMatcher = null;
    let currentStream = null;
    let videoDevices = [];
    let currentDeviceIndex = 0;

    async function loadModels() {
      const MODEL_URL = 'https://cdn.jsdelivr.net/gh/justadudewhohacks/face-api.js/weights';
      await faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL);
      await faceapi.nets.faceRecognitionNet.loadFromUri(MODEL_URL);
      await faceapi.nets.faceLandmark68Net.loadFromUri(MODEL_URL);
    }

    async function getCameras() {
      const devices = await navigator.mediaDevices.enumerateDevices();
      videoDevices = devices.filter(d => d.kind === 'videoinput');
    }

    async function startCamera(deviceId) {
      if (currentStream) {
        currentStream.getTracks().forEach(track => track.stop());
      }
      const constraints = {
        video: deviceId ? { deviceId: { exact: deviceId } } : true
      };
      currentStream = await navigator.mediaDevices.getUserMedia(constraints);
      video.srcObject = currentStream;
      await video.play();

      overlay.width = video.videoWidth;
      overlay.height = video.videoHeight;
    }

    async function start() {
      await loadModels();
      await getCameras();
      await startCamera(videoDevices[currentDeviceIndex]?.deviceId);

      status.textContent = "Models loaded! Add friend's face.";
      runRecognitionLoop();
    }

    async function runRecognitionLoop() {
      if (video.paused || video.ended) return setTimeout(runRecognitionLoop, 100);

      const detections = await faceapi.detectAllFaces(video, new faceapi.TinyFaceDetectorOptions())
        .withFaceLandmarks()
        .withFaceDescriptors();

      ctx.clearRect(0, 0, overlay.width, overlay.height);

      if (detections.length > 0) {
        const resizedDetections = faceapi.resizeResults(detections, { width: overlay.width, height: overlay.height });

        resizedDetections.forEach(detection => {
          const box = detection.detection.box;
          let label = "Unknown";

          if (faceMatcher) {
            label = faceMatcher.findBestMatch(detection.descriptor).toString();
          }

          ctx.strokeStyle = 'green';
          ctx.lineWidth = 3;
          ctx.strokeRect(box.x, box.y, box.width, box.height);
          ctx.fillStyle = 'green';
          ctx.font = '18px Arial';
          ctx.fillText(label, box.x, box.y > 20 ? box.y - 5 : box.y + 15);
        });
      }

      requestAnimationFrame(runRecognitionLoop);
    }

    addFaceBtn.addEventListener('click', async () => {
      const name = nameInput.value.trim();
      if (!name) {
        alert("Enter a name first!");
        return;
      }

      status.textContent = "Detecting face for " + name + "...";

      const detection = await faceapi.detectSingleFace(video, new faceapi.TinyFaceDetectorOptions())
        .withFaceLandmarks()
        .withFaceDescriptor();

      if (!detection) {
        status.textContent = "No face detected. Try again.";
        return;
      }

      const newDescriptor = detection.descriptor;
      labeledDescriptors.push(new faceapi.LabeledFaceDescriptors(name, [newDescriptor]));
      faceMatcher = new faceapi.FaceMatcher(labeledDescriptors, 0.5);

      status.textContent = `Added face for ${name}. Now show faces to recognize!`;
      nameInput.value = "";
    });

    switchCamBtn.addEventListener('click', async () => {
      currentDeviceIndex = (currentDeviceIndex + 1) % videoDevices.length;
      await startCamera(videoDevices[currentDeviceIndex]?.deviceId);
      runRecognitionLoop();
    });

    start();
  </script>
</body>
</html>
